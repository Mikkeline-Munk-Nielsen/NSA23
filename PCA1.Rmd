---
title: "PCA I"
subtitle: "Nyere statistiske analysestrategier F23"
#author: "Mikkeline Munk Nielsen"
institute: ""
#date: "2016/12/12 (updated: `r Sys.Date()`)"
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: xaringan-themer.css
    nature:
      highlightLanguage: r
      highlightStyle: zenburn
      highlightLines: true
      countIncrementalSlides: false
      ratio: '16:9'

---
# Program for i dag

- Introduktion til PCA

- Overblik over processerne i PCA

- Principperne bag PCA

- Forudsætninger for PCA

- Bestemmelse af antal komponenter (det kommer vi til)


---
# PCA forløbet

Vi kommer til at arbejde med faktoranalyse i tre steps over de næste par uger:


- Introduktion til faktoranalyse, herunder PCA som vi skal arbejde med i praksis

- Hvordan gør man PCA (hoveddelen af vægten i kurset):
  + Teori, operationalisering og variabeludvælgelse
  + Identificer antallet af komponenter
  + Rotation
  + Fortolkning af resultater (factor loadings)
  + Videregående analyser
  
- Videregående FA og skalakonstruktion


---
# Introduction til faktoranalyse og PCA


**Disclaimer!** Faget fokuserer på PCA, som er den mest grundlæggende form for *eksporativ* faktoranalyse

- Faktoranalyser er dog en bred gren med mange teknikker - også konfirmative fremfor eksplorative

- Vi starter med en bred introduktion til faktoranalysen i sin helhed

---
# Introduction til faktoranalyse og PCA

.pull-left[
Faktoranalyse referer til en række af teknikker, der forsøger at **reducere variansen** i en korrelationsmatrice med mange (relaterede) variable til få grundlæggende variable

Wikipedia’s definition af faktoranalyse:<break>

*”Factor analysis is a statistical method used to describe variability among observed, correlated variables in terms of a potentially lower number of unobserved variables called factors.“ *
]
.pull-right[![](https://media.sproutsocial.com/uploads/2017/03/How-to-Build-a-Social-Media-Marketing-Funnel-That-Converts.png)]
---

# Let's rewind!
**Hvad er forskellen på LCA og PCA?**

.pull-left[
**LCA** gruppérer **personer** i nogle få grundlæggende *typer/klasser* 
<break>
![](https://www.umass.edu/family/sites/default/files/events/latent_class_analysis_image.png)
]


.pull-right[
**PCA** grupperer **variable** der måler det samme og omgrupperer dem til færre *nye variable*, der opsummerer variationen på tværs af de gamle variable

![](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcR41fy74OEaqmKaGalRK5yIihM9hFBfIUv27A&usqp=CAU)
]

Faktoranalyse handler altså om at gruppere variable på baggrund af, om de i virkeligheden "måler det samme" og derfor "ligner hinanden".

---
# Historisk baggrund


.pull-left[
Personen bag faktoranalyse siges normalt at være statistikeren .link-style2[[Charles Spearman (1863-1945)](https://en.wikipedia.org/wiki/G_factor_(psychometrics)]


Spearman var optaget af ideen om G-faktoren, dvs. at en række menneskelige evner lader til at stamme fra en enkelt evne (**g**eneral intelligence).

”Kan man måle denne underliggende (og uobserverede) G-faktor?”
]
.pull-right[

<img src="https://o.quizlet.com/VIcrsTFAzl0uJKh4xSXcbQ.png"
     width="500px" height="500px"
     style="position:absolute; right:60px; top:110px;">
]

---
# Faktoranalysen som målemodel
Faktoranalysen kan motiveres ud fra en målemodel (dog med modifikationer, som vi vender tilbage til)

.pull-left[
![](Maalemodel.png)
]

.pull-right[
Pilene i målemodellen siger noget om, hvordan den latente variable **F**, påvirker indikatorerne, **I**.


Der er en underliggende antagelse om, at de manifeste indikatorer er betinget uafhængige, givet den latente variabel.

Pilene svarer til det, vi i faktoranalyse kalder *factor loadings* (det vender vi tilbage til)
]

---
# Faktoranalyse som målemodel

Fiktivt eksempel:


- Vi har stillet en gruppe af unge 10 forskellige spørgsmål om deres selværd målt på en skala fra 0-100

- Målet er at måle deres *overordnede* selvværd, dvs. et *latent* fænomen, som vi tror på "styrer" deres svar på de 10 spørgsmål

- Vi kan bruge faktoranalysetil at konstruere et nyt mål for overordnet selvværd, baseret på deres svar på de forskellige spørgsmål.

<break>

.center[
*Men hvordan får vi kombineret informationen fra disse 10 spørgsmål til en meningsfuld skala?*
]

---
# Skalakonstruktion

Man bruger blandt andet faktoranalysen til skalakonstruktion!



En skala er normalt defineret ved at være en (vægtet) sum af variable, der måler det samme underliggende koncept.Skalaen skulle gerne måle et underliggende fænomen eller teoretisk begreb, fx en grundholdning.

<break>


Eksempel på en simpel sumskala med fem items (variable):
$$S = I_i + I_2+I_3+I_4+I_5$$
Eksempel på en *vægtet* sumskala med 5 items:

$$S = 0.5*I_i + 0.9*I_2+1*I_3+0.7*I_4+0.5*I_5$$
---

# Skalakonstruktion

**Summeøvelse i 2 min med sidemanden:** hvad er intuitionen i en vægtet sumskala? Hvad siger vægtene?

Eksempel på en *vægtet* sumskala med 5 items:

$$S = 0.5*I_i + 0.9*I_2+1*I_3+0.7*I_4+0.5*I_5$$
*I en vægtet sumskala tildeles items forskellige vægt på baggrund af deres relevans. Det kan faktoranalysen hjælpe os med!*


---
# Faktoranalyse som målemodel

Faktoranalyse kan faktisk forstås ud fra linear regression.

.pull-left[
"Modellen" er:
$$I_1=\beta_1F+\epsilon_1$$
$$I_2=\beta_2F+\epsilon_2$$
$$I_3=\beta_3F+\epsilon_3$$
]
.pull-right[
Beta-koefficienterne siger noget om, hvor *stærkt* F påvirker responser på indikatorerne.

Betinget uafhængighed betyder, at:
$$corr(\epsilon_1, \epsilon_2, \epsilon_3)=0$$
]

---
# Faktoranalyse som målemodel

Forestil jer en målemodel af følgende form: 
$$y_i = \beta_{1}I_{1i} + \beta_{2}I_{2i} + ... + \beta_{k}I_{ki}$$

PCA producerer nye komponenter/variable, som man kan tænke på som $y_i$ i modellen ovenfor. Disse komponenter konstrueres på baggrund af en vægtet sumskala af vores indikatorvariable (de ti spørgsmål).

PCA vælger $\beta$-værdier sådan at $Var(y_i)\rightarrow \max$. Dvs den nye variabel indeholde så meget af variationen på tværs af indikatorvariablene som muligt!
---
# Faktoranalyse som målemodel

.left-column[
Illustrationen her viser, at den maksimale varians og minimale målefejl findes samtidig, når den roterende linje når de magentafarvede linjer.

]

.right-column[

```{r, echo = FALSE, out.width='100%'}
knitr::include_graphics('https://i.stack.imgur.com/Q7HIP.gif')
```

.center[.backgrnote[Source: ["amoeba" on Stackexchange](https://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues/2700)]]
]


---
# Faktoranalyse kan bruges til...

- At vurdere om det giver mening at lægge variable sammen i én skala

- At tildele vægte til den "vægtede" sumskala - indiatorer, der er særlig konstitutiv for den latente variabel, bliver tildelt størst vægt

- Skalavalidering (særligt CFA)

- Identificere om et konstrukt/begreb er en- eller flerdimensionelt
+ Er solidatitet f.eks. en- eller todimensionelt?

---
# Der er flere typer af faktoranalyser...

Faktoranalyse dækker over både eksplorative og konfirmative teknikker

.pull-left[
**Eksplorativ Ffaktoranalyse (EFA)**
- Principal component analysis (PCA) 
- Common factor model (CFM)
]
.pull-right[
**Konfirmativ (CFA)**
- Konfirmativ faktoranalyse
]



EFA, hvor PCA er den fundamentale teknik, gør sig i princippet ingen antagelser om, hvordan de underliggende variable ser ud (eller hvor mange der er). Er *ikke* modelbaseret.

CFA antager på baggrund af teori, hvad de latente variable *bør* måle. *Er* modelbaseret. 


---
# Der er flere typer af faktoranalyser...

$\Rightarrow$ Fokus i dette kursus er på PCA!

Hvorfor? Fordi kurset handler om data-mining, og fordi det er langt mere praktisk og intuitivt end CFA. Hvis vi får tid berører vi overfladisk CFA, når vi ser på noget af det videregående.

Eksplorativ faktoranalyse referer til ikke-modelbaseret faktoranalyse

I virkeligheden er der to tilgange

- Principal component analysis (PCA)
- Common faktor model (CFM)


---
# PCA vs. CFM

PCA er en statistisk metode, der transfomerer en korrelationsmatrix med mange indikatorvariable til relativt få variable, der er ukorrelerede med hinanden, og som kan forklare den variation, som indikatorvariablene deler. 

.center[
![](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcR41fy74OEaqmKaGalRK5yIihM9hFBfIUv27A&usqp=CAU)
]

*Intuitionen er: PCA sorterer i den variation, som en række manifeste indikatorer deler.*

---
# PCA vs. CFM

CFM baserer sig på samme statistiske teknik som PCA, men antager, at de latente variable (faktorerne) er hypotetiske konstrukter, ikke "virkelige" faktorer.


Forskellen er, at CFM antager, at de latennte variable ikke kan forklare *al* variationen i korrelationsmatricen. 

Det gør PCA derimod (rå datamining)


CFM er altså ikke en deterministisk model; den tager højde for tilfældig støj (unik varians vs. error varians)

Selvom distinktionen er vigtig at forestå, fokuserer kurset udelukkende på PCA. PCA er mere udbredt og nemmere at forstå (og CFM er egentlig bare PCA med et lille tvist)

> Forskellen på PCA og CFm er godt beskrevet i pensum, se Pett et al. 2003:102ff.

---
# PCA HOW TO

**1. Teori + operationalisering (variabeludvælgelse)**
+ Udvælg indikatorer, som man med rimelighed vil forvente måler det, I gerne vil måle.

+ Motivation: Hvilke variable tror jeg ”måler det samme”?

+ Eller: Består et begrebet af en eller to dimensioner? <break>

**2. Identificer antallet af faktorer**
+ Hvordan man gør det i praksis, vender vil tilbage til… <break>

**3. Roter dine faktorer (!)**
+ En operation, der gør dine factor loadings fortolkelige (vender vi også tilbage til)<break>

---
# PCA HOW TO

**4. Beskriv dine resultater**
+ - med udgangspunkt i de roterede factor loadings. Psst... Factor loadings svarer lidt til rho-parametre i LCA

**5. Beskriv dine resultater**
+ Brug de forudsagte latente variable i videre analyser. Er der kønsforskelle i institutionel tillid?
Eller konstruér simple skalaer/indeks med udgangspunkt i dine faktoranalyses resultater og brug disse skalaer i videre forskning

---
# Baggrund for PCA

PCA er en statistisk metode, der transformerer en korrelationsmatrix med mange variable til få variable, der er ukorrelerede med hinanden.

Intuitionen: PCA sorterer i alt den variation,som en række manifeste indikatorer deler.


Lad os prøve at se på et eksempel fra European social survey omkring generaliseret tillid:
---
# Baggrund

```{r, include=FALSE}

# Read dataset (for now as dta file, which Haven can help with)

ESS_DK <- read_dta("C:/Users/mmn/ROCKWOOL Foundation Dropbox/Mikkeline Munk Nielsen/Nyere statistiske analysestrategier F23/slides-mmn/PCA I/ESS6DK.dta")

ESS_DK <- ESS_DK %>%
  zap_labels() %>% 
  transmute(
    #Openness indicators (coding missing values):
    parliament = if_else(trstprl %in% c(88:99), NA_real_, trstprl),
    legal_system = if_else(trstlgl %in% c(88:99), NA_real_, trstlgl),
    police = if_else(trstplc %in% c(88:99), NA_real_, trstplc),
    politicians = if_else(trstplt %in% c(88:99), NA_real_, trstplt),
    parties = if_else(trstprt %in% c(88:99), NA_real_, trstprt),

    #Education variables:
    edu = case_when(
      edulvlb %in% c(8888:9999) ~ NA_real_,
      edulvlb %in% c(0:600) ~ 0,
      TRUE ~ 0,
      edulvlb %in% c(601:801) ~ 1,
      TRUE ~ 1)) %>%
  #Restricting sample to non-missing:
  drop_na()

```
Eksempel på en korrelationsmatrice
```{r echo=FALSE}

(ESS_DK %>% 
    dplyr::select(parliament, legal_system, police, politicians, 
                  parties) %>% 
    cor())


```

Alle variable korrelerer positivt, dog med varierende styrke. 
Tillid til regeringen korrelerer f.eks. højt med tillid til politikere og politiske partier, men kun moderat med tillid til politiet!

*Men kan matricen forklaren af èn underliggende variabel, der kan siges at måle institutionel tillid som teoretisk begreb? Lad os prøve at køre en PCA og se ad!* 

---
# Principperne bag PCA
Lad os prøve at køre en PCA og se ad!

PCA forsøger at ”mine” korrelationsmatricen ved at se, om alle korrelationer kan forklares af en (lineær) kombination af underliggende, ukorrelerede variable kaldet komponenter.

Og bemærk…

PCA er ikke modelbaseret – det er bare en iterativ statistisk metode, som dekomponerer korrelationsmatricer vha. ortogonale transformationer.
Det er arbejdshesten i eksplorativ faktoranalyse.

---
# Principperne bag PCA

PCA baserer sig på eigenvalue decompositions, dvs. dekompositioner af en korrelationsmatrix ud i dens egenværdier og egenvektorer.


**Princippet er som følger:**

**1)** Identificer den første komponent, der forklarer så meget fællesvariation som muligt i korrelationsmatricen.

**2)** Find da den anden komponent, der forklarer så meget som muligt af fællesvariation, der er tilbage i matricen efter man har trukket den første komponent ud!

**3)**  som i punkt 2 indtil der ikke er mere fællesvariation at forklare.



*Bemærk: Man skal ikke ku’ de matematiske principper til eksamen, men for interesserede er det fornuftigt at gennemgå de centrale principper.*
---
# Principperne bag PCA
Det helt centrale i PCA-metoden er, at man udtrækker eller fjerner den forrige komponent førend man identificerer den næste komponent. 

Der foregår altså en løbende ”residualisering” af korrelationsmatricen.
Det er det samme som at sige, at komponenterne er ukorrelerede med hinanden. De er en ”lineær kombination af ortogonale variable”.

Dette princip i PCA betyder også, at den første komponent altid er den komponent, der forklarer mest variation, den anden næstmest, osv.…
---
# Principperne bag PCA

```{r, echo=TRUE}

pca <- (ESS_DK %>% 
          dplyr::select(parliament, legal_system, police, politicians, 
                  parties) %>% 
          prcomp(center=TRUE, scale=TRUE))

summary(pca)

```
PCA er kendetegnet ved, at den fortsætter med at finde komponenter ind til al variation i korrelationsmatricen er forklaret.
Det betyder også, at PCA altid finder lige så mange komponenter, som der er indikatorvariable!
Her adskiller PCA sig fra CFM – CFM antager, at man ikke kan forklare al variation…

Komponenterne er ordnet efter hvor meget variation/information de fanger hver især (læg mærke til at første komponent forklarer mest, anden komponent lidt mindre osv).
---
# Principperne bag PCA

Componenternes standardafvigelser (SD) fortæller os om vigtigheden at hvert komponent.
```{r, echo=TRUE}
summary(pca)
```
Hvis vi kvardrerer dem, får vi de såkaldte eigenværdier!
```{r, echo=TRUE}
(pca$eigen_values <- pca$sdev^2)

```

---
# Forudsætninger for PCA
Lærebøger (fx Hansen 2017 eller Pett et al.-bogen) fremhæver typisk, at man skal undersøge sin korrelationsmatrice, inden man går i gang med en PCA (eller anden EFA).

Spørgsmålet er, om indikatorvariablene deler nok variation til at det giver mening at kombinere dem. Sagt på en anden måde: *er korrelationsmatricen ”faktorabel”?*

Vi kan godt være lidt skeptiske overfor formelle regler for, hvornår en matrice er faktorabel – men det er altid godt at tjekke sine data, inden man går videre i analyserne!
---
# Forudsætninger for PCA
Hansen (2017) siger, at hver indikatorvariabel i en korrelationsmatrix skal have mindst en korrelation på over 0,3.

Hvis dette ikke er tilfældet, vil item’et sjældent kunne bidrage til at fortolke faktorerne.

Disse items kan enten fjernes, eller man kan begrunde at tage dem med ud fra teoretiske argumenter og så være særlig opmærksomme på dem i analysen.

** 2 min med sidemanden: opfylder korrelationsmatricen fra før denne betingelse?**
```{r echo=FALSE}

(ESS_DK %>% 
    dplyr::select(parliament, legal_system, police, politicians, 
                  parties) %>% 
    cor())
```
---
# Forudsætninger for PCA

Pett et al. fremhæver to formelle tests:

**1)** Kaiser-Meyer-Olkin test (KMO)

**2)** Bartlett’s (sfæriske) test
---
# Forudsætninger for PCA - KMO

- Undersøger korrelationer parvist og sammenligner dem med de partielle korrelationer (når der kontrolleres for øvrige items).

- Hvis items er en funktion af underliggende faktorer, vil partielle korrelationer være små sammenlignet med ujusterede korrelationer.

- Ved værdier over 0,8 anses matricen for at være tilstrækkelig faktorabel. 

```{r echo=TRUE, message=FALSE, warning=FALSE}

library(psych) # library der indeholder KMO-funktion og funktion til bartletts-test
(ESS_DK %>% 
    dplyr::select(parliament, legal_system, police, politicians, parties) %>% 
    KMO()) # KMO funktion
```

---
# Forudsætninger for PCA

** Bartlett’s (sfæriske) test **

- Tester om korrelationsmatricen er en såkaldt ”identitetsmatrix”, hvilket er ensbetydende med, at alle variable er ukorrelerede. 

- Små p-værdier (under 0,05) indikerer, at matricen er velegnet til faktoranalyse (forkaster H0, som er uafhængighed).
```{r echo=TRUE, message=FALSE, warning=FALSE}
(ESS_DK %>% 
    dplyr::select(parliament, legal_system, police, politicians, parties) %>% 
    cortest.bartlett(n=1406)) # KMO funktion, argument n= antal observationer
```
---
# Forudsætninger for PCA
- Disse tests siger noget om, hvor velegnet en korrelationsmatrice er til PCA (og FA generelt).

- De skal og bør bruges heuristisk – ofte er teoretiske argumenter for medtagelse af indikatorer lige så centrale, hvis ikke vigtigere!

- De to formelle tests kan bruges til at rette sin opmærksomhed mod nogle specifikke items, der måske opfører sig anderledes end ventet.

---
# Hvor mange komponenter?

Vi så lige før, at PCA producerer lige så mange komponenter som der er indikatorvariable. Men da hele pointen med PCA er at reducere antallet af variable, hvordan ved vi så, hvor mange vi skal beholde?

I modsætning til LCA, er der ikke noget modelbaseret svar i PCA.

Derfor bruger man en eller flere selektionsregler, dvs. nogle principper for, hvor mange komponenter, man skal fastholde.

---
# Hvor mange komponenter?

Litteraturen fremhæver tre selektionsregler:

**1)** Egenværdier > 1 (Kaiser-Guttman kriteriet).

**2)** Cattell’s scree test (scree plot).

**3)** Parallel analysis (de facto standard i dag).


Bemærk: Pett et al. fremhæver kun 1+2, da 3 er relativt ny. Hayton et al. (2004) beskriver meget fint 3.

---
# Hvor mange komponenter?

** Kaiser-Guttman kriteriet: behold eigenværdier > 1**

- Rationale: Egenværdier større end 1 indikerer, at komponenten kan forklare mere end, hvad én indikatorvariabel kan forklare (og det giver jo ikke mening at beholde noget, der forklarer mindre end de oprindelige variable...)

```{r}
(pca$eigen_values) # print eigenværdier
```

Ulemper ved Kaiser-Guttman kriteriet: 
- Tendens til at overestimere antal reelle faktorer… 
- Arbitrært at skelne lige ved 1: hvad hvis man har eigenværdier på 0.98 or 1.01? 
---
# Hvor mange komponenter?

** Scree plot **

```{r ref.label = "plot-pca2", echo = FALSE, out.width='100%', fig.height = 4, fig.width = 5}
# Visualize PCA object
fviz_pca_var(pca) 
```

---
# Hvor mange komponenter?

** Parallel analysis: simulationsbaseret teknik**

- Man simulerer et nyt datasæt med 
+ samme antal observationer
+ samme antal variable, men hvor
+ variablene er ukorrelerede med hinanden.
- Man kører en PCA på de simulerede data og laver et scree-plot.
- Man kører en PCA på de ”ægte” data, laver et scree-plot, og ”lægger” plottet ned over det simulerede plot.
- Antallet af signifikante komponenter er de komponenter, der ligger over skæringspunktet mellem de to kurver.

```{r eval=FALSE}
library(paran)
(ESS_DK %>% 
  dplyr::select(parliament, legal_system, police, politicians, parties) %>% 
  paran(graph=T))
```
---
# Hvor mange komponenter?

.center[
![](Paran.png)
]
** Parallel analysis: simulationsbaseret teknik**
---
class: clear
# PCA plot test

.pull-left[
```{r plot-pca, fig.show = "hide"}
# Visualize PCA object
fviz_pca_var(pca)
```

*Interpretation*: As our first principal component increases by one, scores on `imbgeco` increase by roughly 1.8.
]

.pull-right[
```{r ref.label = "plot-pca", echo = FALSE, out.width='100%', fig.height = 4, fig.width = 5}
```
]
