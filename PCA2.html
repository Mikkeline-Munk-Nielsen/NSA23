<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>PCA II</title>
    <meta charset="utf-8" />
    <meta name="author" content="v. Mikkeline Munk Nielsen" />
    <script src="libs/header-attrs-2.19/header-attrs.js"></script>
    <link href="libs/tile-view-0.2.6/tile-view.css" rel="stylesheet" />
    <script src="libs/tile-view-0.2.6/tile-view.js"></script>
    <link href="libs/tachyons-4.12.0/tachyons.min.css" rel="stylesheet" />
    <link href="libs/panelset-0.2.6/panelset.css" rel="stylesheet" />
    <script src="libs/panelset-0.2.6/panelset.js"></script>
    <link rel="stylesheet" href="xaringan-themer.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

.title[
# PCA II
]
.subtitle[
## Nyere statistiske analysestrategier F23
]
.author[
### v. Mikkeline Munk Nielsen
]

---


<div>
<style type="text/css">.xaringan-extra-logo {
width: 220px;
height: 256px;
z-index: 0;
background-image: url(https://designguide.ku.dk/download/co-branding/ku_co_uk_h.jpg);
background-size: contain;
background-repeat: no-repeat;
position: absolute;
top:1em;right:1em;
}
</style>
<script>(function () {
  let tries = 0
  function addLogo () {
    if (typeof slideshow === 'undefined') {
      tries += 1
      if (tries < 10) {
        setTimeout(addLogo, 100)
      }
    } else {
      document.querySelectorAll('.remark-slide-content:not(.title-slide):not(.inverse):not(.hide_logo)')
        .forEach(function (slide) {
          const logo = document.createElement('div')
          logo.classList = 'xaringan-extra-logo'
          logo.href = null
          slide.appendChild(logo)
        })
    }
  }
  document.addEventListener('DOMContentLoaded', addLogo)
})()</script>
</div>

# Program for i dag

- Recap

  + PCA principper
  
  + Faktorabilitet
  
  + Antal komponenter
  
  + Egenværdier og egenvektorer
  

- Fortolkning af komponenter - *factor loadings*

- Rotation

- Øvelser!


---
# Recap


Sidste gang lærte vi at

--

- PCA er en statistisk metode, der transformerer en korrelationsmatrice med mange (korrelerede) variable til få variable, der er ukorrelerede med hinanden 

--

- PCA forsøger at ”mine” denne korrelationsmatrice ved at se, om alle korrelationer kan forklares af en (lineær) kombination af underliggende, ukorrelerede variable kaldet komponenter.

--

- PCA baserer sig på eigenvalue decompositions, dvs. dekompositioner af korrelationsmatricen ud i dens egenværdier og egenvektorer.


---
# Recap

- Måde hvorpå man kan reducere ”mange variable” til ”få grundlæggende variable” som opsummerer informationen i de ”mange variable”.

--

- Praktisk til at måle et svært observerbare fænomener eller begreber, fx en grundholdning, tillid, personlighed.

--

- Kan hjælpe med at undersøge om et fænomen/begreb er en- eller flerdimensionelt. Er fx jobtilfredshed 1- eller 2-dimensionelt?

--

  + Hvordan måler vi jobtilfredshed? 

--
  
  + Svært at observere – eller måle med ét spørgsmål

---
# Eksempel: jobtilfredshed
  
.center[
![](Job_satifaction_1.jpg)
]

---
# Eksempel: jobtilfredshed
  
.pull-left[
    ![](Job_satifaction_2.jpg)
]

.pull-right[
  ![](Job_satifaction_3.jpg)
]


---
# Eksempel: jobtilfredshed

.pull-left[
    
- Hvis vi undersøger korrelationsmatricen kan vi altså se, at der ser ud til at være to grupper af høje korrelationer mellem batteriets items. 
    
    
- Givet de items som hver gruppe bygger på, hvad kunne vi så tolke, at den hhv. blå og røde gruppe af varibale siger noget om?
]


.pull-right[
  ![](Job_satifaction_2.jpg)
]

---
# Eksempel: jobtilfredshed

.pull-left[
    
- Hvis vi undersøger korrelationsmatricen kan vi altså se, at der ser ud til at være to grupper af høje korrelationer mellem batteriets items. 
    
    
- Givet de items som hver gruppe bygger på, hvad kunne vi så tolke, at den hhv. blå og røde gruppe af varibale siger noget om?
]


.pull-right[
  ![](Job_satifaction_4.jpg)
]
---
# Recap


PCA forsøger at ”mine” korrelationsmatricen ved at se, om alle korrelationer kan forklares af en (lineær) kombination af underliggende, ukorrelerede variable kaldet komponenter.

--

***Det metodiske princip i PCA er følgende:***

--

**1)** Identificer den første komponent, der forklarer så meget fællesvariation som muligt i korrelationsmatricen.

--

**2)** Find da den anden komponent, der forklarer så meget som muligt af fællesvariation, der er tilbage i matricen efter man har trukket den første komponent ud!

--

**3)** Fortsæt som i punkt 2 indtil der ikke er mere fællesvariation at forklare.

---
# Recap

**MEN!** 

Sidst gang lærte vi, at forudsætningen for PCA er, at der er høj korrelation mellem de items, som man kører PCA på. Korrelationsmatrien skal så at sige være ***faktorabel***.

--

- Underesøg korrelationer mellem variable: Hver indikatorvariabel i en korrelationsmatrice skal have mindst en korrelation på over 0,3 (Hansen 2017)

--

- Kaiser-Meyer-Olkin test (KMO)

--

- Bartlett’s (sfæriske) test

--

Disse tests siger noget om, hvor velegnet en korrelationsmatrice er til PCA (og FA generelt). Vi lærte dog også, at de skal og bør bruges heuristisk – ofte er teoretiske argumenter for medtagelse af indikatorer lige så centrale, hvis ikke vigtigere!

---
# Recap 

Fordi PCA forklarer al fællesvariationen i korrelationsmatricen finder den altid lige så mange komponenter, som der er indikatorvariable!

--

*Hvor mange komponenter beholder vi?* Ofte vil en god PCA være kendetegnet ved, at den reducerer antallet af variable, men fastholder de komponenter forklarer en betydelig del af den samlede variation eigenvalues

--

***Litteraturen fremhæver tre selektionsregler:***

--

**1)** Egenværdier &gt; 1 (Kaiser-Guttman kriteriet).

--

**2)** Cattell’s scree test (scree plot).

--

**3)** Parallel analysis (de facto standard i dag).

---
# Recap
Med andre ord lærte vi sidste gang:

**1)** At undersøge, om en korrelationsmatrice er faktorabel

**2)** At køre en pca i R

**3)** At bestemme, hvor mange komponenter vi skal beholde

--

*I dag skal vi se nærmere på komponenterne og lære at fortolke deres indhold.*

---
# PCA i praksis

Sidst arbejdede vi med eksemplet omkring institutionel tillid. Korrelationmatricen (5x5) så således ud:




```
##              parliament legal_system    police politicians   parties
## parliament    1.0000000    0.5584236 0.3784972   0.7224152 0.6775825
## legal_system  0.5584236    1.0000000 0.5763997   0.4839359 0.4645301
## police        0.3784972    0.5763997 1.0000000   0.3742397 0.3602119
## politicians   0.7224152    0.4839359 0.3742397   1.0000000 0.8380581
## parties       0.6775825    0.4645301 0.3602119   0.8380581 1.0000000
```

Og vi kørte en PCA med **`prcomp()`** functionen

```r
pca &lt;- (ESS_DK %&gt;% 
          dplyr::select(parliament, legal_system, police, politicians, 
                  parties) %&gt;% 
          prcomp(center=TRUE, scale=TRUE))
```

---
# PCA i praksis

**`Summary()`** funktionen gav os komponenternes standardafvigelser (SD), som fortæller os om vigtigheden at hvert komponent.

```r
summary(pca)
```

```
## Importance of components:
##                           PC1    PC2     PC3     PC4     PC5
## Standard deviation     1.7901 0.9472 0.65924 0.55306 0.39738
## Proportion of Variance 0.6409 0.1794 0.08692 0.06118 0.03158
## Cumulative Proportion  0.6409 0.8203 0.90724 0.96842 1.00000
```

--

Hvis vi kvardrerer dem, får vi de såkaldte eigenværdier!

```r
(pca$eigen_values &lt;- pca$sdev^2)
```

```
## [1] 3.2044837 0.8971327 0.4345971 0.3058763 0.1579102
```

--

***Egenværdier*** repræsenterer komponenternes forklaringskraft, dsv. hvor meget af variationen i korrelationsmatricen, som en komponent fanger


---
# PCA i praksis

**`prcomp()`** outputtet gav os fem komponenter og deres tilhørende egenvektorer:

```r
pca
```

```
## Standard deviations (1, .., p=5):
## [1] 1.7901072 0.9471709 0.6592398 0.5530609 0.3973792
## 
## Rotation (n x k) = (5 x 5):
##                     PC1        PC2        PC3        PC4         PC5
## parliament   -0.4757480 -0.1972727 -0.3697872  0.7620959  0.13120421
## legal_system -0.4217429  0.4622473 -0.6373561 -0.4491985 -0.02140483
## police       -0.3536159  0.6994720  0.5740129  0.2367686  0.01201909
## politicians  -0.4907882 -0.3524476  0.2112896 -0.1659430 -0.75015153
## parties      -0.4794576 -0.3659652  0.2879242 -0.3658333  0.64765385
```

--

***Egenvektorer*** er kollonner af vægte, der beskriver sammenhængen mellem de originale items og nye komponenter.

--

I dag tager vi udgangspunkt i egenvektorerne, når vi skal vi arbejde med *factor loadings!*


---
# Factor loadings

**Definition**: *En factor loading er korrelationen mellem en komponent og en indikatorvariabel. Vi bruger dem til at fortolke komponenterne!*

--

Factor loadings kommer ud af den dekomposition, som PCAen laver:

--

  + ***Egenværdier*** repræsenterer komponenternes forklaringskraft, dsv. hvor meget af variationen i korrelationsmatricen, som en komponent fanger

--

  + ***Egenvektorer*** er kollonner af vægte, der beskriver sammenhængen mellem de originale items og nye komponenter. 

--

Factor loadings defineret som produktet mellem kvadratroden af egenværdien og en egenvektor

--

*(OBS: Det skal man ikke redegøre for til eksamen! Se Appendix E i Pett et al. (2003) eller Kline (1994) pp. 28-41)*

---
# Factor loadings

Vi kan udregne factor loadings og gemme dem i en matrice ved at gange kvardratroden af hver komponents egenværdi på komponentens egenvektor:


```r
(pca$factor_loadings &lt;- pca$rotation*t(matrix(rep(sqrt(pca$eigen_val),5),nrow=5,ncol=5))
) # Læg mærke til at jeg har specificeret matricens dimensioner (5x5) 
```

```
##                     PC1        PC2        PC3        PC4          PC5
## parliament   -0.8516399 -0.1868509 -0.2437785  0.4214855  0.052137823
## legal_system -0.7549651  0.4378272 -0.4201705 -0.2484341 -0.008505834
## police       -0.6330103  0.6625195  0.3784122  0.1309475  0.004776135
## politicians  -0.8785634 -0.3338281  0.1392905 -0.0917766 -0.298094603
## parties      -0.8582804 -0.3466316  0.1898111 -0.2023281  0.257364157
```
--

Disse tal repræsenterer korrelationen mellem de originale items og de nye komponenter/variable, som PCA'en producerer. 

---
# Factor loadings

Vi bruger factor loadings til at fortolke indholdet i de nye variable, som vi har konstrueret. 


```
##                     PC1        PC2        PC3        PC4          PC5
## parliament   -0.8516399 -0.1868509 -0.2437785  0.4214855  0.052137823
## legal_system -0.7549651  0.4378272 -0.4201705 -0.2484341 -0.008505834
## police       -0.6330103  0.6625195  0.3784122  0.1309475  0.004776135
## politicians  -0.8785634 -0.3338281  0.1392905 -0.0917766 -0.298094603
## parties      -0.8582804 -0.3466316  0.1898111 -0.2023281  0.257364157
```

--

Fremgangsmåden er, at vi fortolker komponenterne på baggrund af, hvor højt de "loader" (eller korrelerer) med hver af de originale items. 

--

Eksemplet her viser, at alle indikatorerne loader forskelligt på de forskellige komponenter. Hvis vi fx antog, at der var 2 komponenter (dvs. to dimensioner i institutionel tillid), så kunne det tyde på, at 

--

**1)** den første komponent måler generel tillid.
  
--

**2)** den anden komponent måler tillid til retssystemet og politiet.

---
# Factor loadings

.pull-left[

- Illustrationen viser de originale items placeret i et koordinatsystem med de to første komponenter

- Dette er de rå factor loadings... men det anbefales, at man ikke fortolker direkte på de rå... i stedet *roterer* man komponenterne, for at lette fortolkningen

]
.pull-right[
&lt;img src="PCA2_files/figure-html/unnamed-chunk-10-1.png" width="100%" /&gt;
]

---
# Rotation

- Formålet med rotation af faktorer er at opnå fortolkelige komponenter/faktorer, dvs. komponenter, hvor factor loadings relativt entydigt hjælper med at beskrive de nye variable.

--

***Hvorfor roterer vi??***

--

- Komponenter i en PCA udspænder et flerdimensionalt rum, hvori de manifeste variable er placeret. 

--

- Intuitionen er, at vi kan anskue det rum, som komponenterne udspænder, fra uendeligt mange vinkler.

--

- Factor loadings vil afhænge af, hvorfra man ser rummet (lidt forsimplet sagt).

--

- Rotation hjælper med at vælge en ”synsvinkel”, som giver en (mere) entydig fortolkning af komponenterne

---
# Rotation

.pull-left[

.center[**Uroteret**]
&lt;img src="PCA2_files/figure-html/unnamed-chunk-11-1.png" width="90%" /&gt;
]







.pull-right[
.center[**Roteret**]

&lt;img src="PCA2_files/figure-html/unnamed-chunk-13-1.png" width="90%" /&gt;
]

--

*(Note, det roterede plot viser ikke de korrekte factor loadings (dem regner vi selv ud), men det viser, hvordan akserne rykker sig)*

---
# Rotation

Matematisk set uendeligt mange rotationsløsninger, så hvilken én skal man vælge?

--

*Man skal vælge den, der giver **simple structure**, dvs. en løsning, der (bedst) tydeliggør fortolkningen af komponenterne ved hjælp af entydige factor loadings!*

---
# Simple structure kriterier

--

**1)** Hver række af faktor loadings skal indeholde mindst én faktor loading på 0.

--

**2)** Hvis der er X komponenter, skal hver komponent have mindst X faktor loadings på 0 (f.eks. tre komponenter, mindst tre 0’er per komponent).

--

*For hvert parvis sammenligning af komponenter, skal der være:*

--

**3)** Mange indikatorer der har faktor loadings på 0 for den ene komponent, men ikke for den anden.

--

**4)** Mange indikatorer som har 0 for begge komponenter (når der er flere end fire komponenter).

--

**5)** Et lille antal ”non-zero” faktor loadings for begge komponenter. 


*Pett et al. (2003:132)*

---
# Simple structure kriterier

Den ideelle rotation resulterer i en simple structure, så: 

--

- Hver indikatorvariabel har en høj loading på én komponent 

--

- Hver komponent har en høj loading på kun nogle af indikatorvariablene

--

Simple structure er det ønskværdige mål at opnå, men ofte er det svært at overholde alle kriterier.

--

I virkelighedens verden er løsningen mere pragmatisk: Vi fortolker komponenter ud fra de højeste factor loadings - også selvom nogle indikatorer korrelerer højt med flere komponenter.


---
# Rotationsmetoder

Der findes mange rotationsmetoder, der hver især forsøger at opnå simple structure ud fra lidt forskellige kriterier. *Den vigtige distinktion her er mellem:*

--

- **ortogonale** (vinkelrette) rotationer. Her roteres akserne (komponenterne), så de er ortogonale på hinanden

--

- **oblique** (skrå) rotationer. Her tillader vi akserne (komponenterne) at være korrelerede.

---
# Rotationsmetoder

Ortogonal rotation er den mest udbredte og den mest intuitive.

--

Den mest bruge rotationsmetode er **varimax**.

--

- Varimax maksimerer variansen på de kvadrerede loadings for hver søjle (komponent) i factor loading matricen.

- Varimax producerer høje og lave loadings inden for hver faktor, jf. definitionen af simple structure.

--

*Der findes andre ortogonale rotationsmetoder, men dem gennemgår vi ikke her.
Varimax er standarden. Se Pett et al. (2003) for flere metoder.*

---
# Orthogonal rotation

.pull-left[

.center[**Uroteret**]
&lt;img src="PCA2_files/figure-html/unnamed-chunk-14-1.png" width="95%" /&gt;
]


.pull-right[
.center[**Orthogonal (varimax)**]
&lt;img src="PCA2_files/figure-html/unnamed-chunk-15-1.png" width="95%" /&gt;
]

--

*(note: det roterede plot viser ikke de korekte faktor loadings. Dem regner vi selv ud)*

--

*(note: det roterede plot viser ikke de korekte faktor loadings. Dem regner vi selv ud)*
---
# Rotationsmetoder

***Oblique rotation*** tillader akserne eller komponenterne at være korrelerede. 

--

- Det er meget avanceret, men det betyder, akserne ikke længere er vinkelrette på hinanden.

--

- Rent geometrisk kan korrelationer nemlig udtrykkes med vinkler. Jo længere væk fra 90 grader, des større korrelation.

--

Begrundelsen for at bruge oblique rotation er, at komponenter ofte vil være korrelerede i den virkelige verden – de er ikke ortogonale på hinanden.

--

Det er et stærkt teoretisk argument! Tænk på dimensioner i lykke fx – hvorfor skulle de ikke være korreleret med hinanden?

--

*(Ofte vil ortogonale og oblique rotationer dog give nogenlunde det samme resultat (mærkeligt fænomen)…*

Vi bruger metoden ***promax***.

---
# Oblique rotation

.pull-left[

.center[**Uroteret**]
&lt;img src="PCA2_files/figure-html/unnamed-chunk-16-1.png" width="95%" /&gt;
]







.pull-right[
.center[**Oblique (promax)**]
&lt;img src="PCA2_files/figure-html/unnamed-chunk-18-1.png" width="95%" /&gt;
]

--

*(note: det roterede plot viser ikke de korekte faktor loadings. Dem regner vi selv ud)*

---
# Sammenligning
.pull-left[
**Jonas** har lavet denne fantastiske (!) animation, der demonstrerer forskellene i rotation med udgangspunkt i de 25 spørgsmål som børns adfærd, som I arbejdede med i øvelsen sidst

]



.pull-right[
&lt;img src="Rotation_viz_BFU.gif" width="150%" /&gt;
.center[.backgrnote[*af Jonas Strøyberg Jensen*]]
]

---
# Rotationsmetoder

Ved oblique rotation skelner man mellem:

--

**1) Factor pattern matrix**: Korrelationen mellem komponenten/faktoren og de manifeste variable, når man har ”kontrolleret for” korrelationen mellem komponenterne. Det er partielle regressionsbetakoefficienter/factor loadings.

--

**2) Factor structure matrix**: De rå korrelationer mellem den roterede komponent/faktor og de manifeste variable. De er ubetingede factor loadings.

--

**3) Factor correlation matrix**: Korrelationerne mellem de komponenterne. Fortæller os, om de rent faktisk er korrelerede!


--

*Så hvilke bruger vi til hvad?*

---
# Rotationsmetoder

- Standarden er at fortolke komponenterne på factor **structure** matrix, da det minder om factor loadings i ortogonal rotation.

--

- Dvs. man finder simple structure i *factor pattern matrix* og fortolker på komponenterne med *factor structure* matrix

--

Personligt er vi dog tilbøjelige til at sige, at begge er vigtige og siger noget forskelligt, men på den anden side viser litteraturen, at de ofte giver samme resultat anyways…

--

- Vi bruger *factor correlation matrix* til at fortolke på, om komponenterne korrelerer

---
# Rotationsmetoder

**`prcomp()`** funktionen giver os kun factor pattern matrix.

For at få structure matrix skal vi gange *factor pattern matrix* på *factor correlation matrix*. 



```r
# Første skridt er at rotere
pca_obli &lt;- Promax(pca$factor_loadings[,1:2])

# Pattern matrix ligger under "loadings"
pca_obli$loadings

# Factor correlation matrix gemmes som "Phi"
pca_obli$Phi

# Structure matrix findes ved at gange de to sammen:
pca_obli$structure &lt;- pca_obli$loadings %*% pca_obli$Phi
```

--

**2 min med sidemanden** *Er der en forskel mellem factor pattern og factor structure matrix ved ortogonal rotation?*

*De snakker om det i Pett et al. (2003)*
---
# Rotationsmetoder

**En sidste vigtig ting** *Hvilken løsning skal jeg rotere? Den fulde løsning eller kun de komponenter, som vi beholder”?*

--

- Anbefalingen er at rotere løsningen med signifikante komponenter.

---
# Program for resten af dagen:

**1)** Pause

**2)** Arbejde videre på øvelsen fra sidst *(Børn er forskellige)*

**3)** Pause

**4)** Ny øvelse! PCA2: Tolerance please!

*I den nye øvelse går vi tilbage til start og laver en PCA til og med rotationer. Næste gang laver vi videregående analyser*

    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightLanguage": "r",
"highlightStyle": "zenburn",
"highlightLines": true,
"countIncrementalSlides": false,
"ratio": "16:9",
"self_contained": true
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
